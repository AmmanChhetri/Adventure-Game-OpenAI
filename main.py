from cassandra.cluster import Cluster
from cassandra.auth import PlainTextAuthProvider
from langchain.memory import CassandraChatMessageHistory, ConversationBufferMemory

# Used to interact with OpenAI
from langchain.llms import OpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
import json

# This secure connect bundle is autogenerated when you download your SCB, 
# if yours is different update the file name below
cloud_config= {
  'secure_connect_bundle': 'secure-connect-choose-your-own-adventure.zip'
}

# This token JSON file is autogenerated when you download your token, 
# if yours is different update the file name below
with open("choose_your_own_adventure-token.json") as f:
    secrets = json.load(f)

CLIENT_ID = secrets["clientId"]
CLIENT_SECRET = secrets["secret"]

ASTRA_DB_KEYSPACE = "database"
OPENAI_API_KEY = ""

auth_provider = PlainTextAuthProvider(CLIENT_ID, CLIENT_SECRET)
cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider)
session = cluster.connect()



# Our Aim is Simple as we are chatting with the AI(our LLM model,,,we are training our own LLM model...based on the given template)
# ...we want the message History to be saved right?...so that we will produce the upcoming promts from AI based on the 
# previous messages...that's why we are providing memory to our LLM using the Cassandra Astra DataBase ...that is very faster 
# compared to normal sql database...as this Cassandra database uses Multi-Dimensional Vectors to store data...which makes it
# way faster... 


# This is just we are storing....a session in the database..so this just represents the particular session of the game....if 
# you want to store the progress of the whole game then...as long as you know the session-id and you are not clearing the 
# message_history...you can access any game session chat history....using session-id...
message_history = CassandraChatMessageHistory(
    session_id = "anything",
    session = session, # Connection to database...as given above...
    keyspace = ASTRA_DB_KEYSPACE,
    ttl_seconds = 3600
)

# we are just clearing the memory ...so what is happending is that whenver we start a new game...the database message_history 
# is deleted completely..and we start to record new message....now let say you were playing the game and left in between or just
# completed the game(you haven't started a new game) ...then the maximum time it will stay in the database is 3600s or 60mins
message_history.clear()

# So what actually happenning is ...whenever you start playing the game....a session of the game with the name "anything" is 
# created in the database...and now whatever chat happens between you and AI will be saved in the memory of the database under
# this session("anything")...now each time we start a new game the old data in the database gets deleted and we start with a 
# fresh memory....data in the database is deleted...as we are using message_history.clear()....also the message_history for this
# session is deleted in 60 mins....


# So we are just taking a particular message_history...and converting it to a form to which the conversation with AI is easier
# and compatible...
cass_buff_memory = ConversationBufferMemory(
    memory_key = "chat_history",
    chat_memory = message_history
)


# So here we are creting our llm(large language model)...and now what we want is it to genereate the dyanimc prompts to the
# user based on its input....but on the basis of given template(here the template is of an adventure game...so it will revolve
# around it)


# template is a way of dynamically injecting data to the CHAT-GPT or LLM's or the Prompts...
# If you see in this template we have provided a {chat_history}...which will be keep on updating....as the chat between AI and
# human continues...and each time it will be given to the prompt....
template = """
You are now the guide of a mystical journey in the Whispering Woods. 
A traveler named Elara seeks the lost Gem of Serenity. 
You must navigate her through challenges, choices, and consequences, 
dynamically adapting the tale based on the traveler's decisions. 
Your goal is to create a branching narrative experience where each choice 
leads to a new path, ultimately determining Elara's fate. 

Here are some rules to follow:
1. Start by asking the player to choose some kind of weapons that will be used later in the game
2. Have a few paths that lead to success
3. Have some paths that lead to death. If the user dies generate a response that explains the death and ends in the text: "The End.", I will search for this text to end the game
4. There should be atleast 10 decision in any path

Here is the chat history, use this to understand what to say next: {chat_history}
Human: {human_input}
AI:""" 

# User can enter as his input anything...and our model will generate a response as a prompt based on this input and
# message_history....
prompt = PromptTemplate(
    # these input_variables are the dynamically changing variables in our prompt....we will feed this input variables to our 
    # template
    input_variables=["chat_history","human_input"],
    template=template
)

# Creating LLM ....
llm = OpenAI(openai_api_key = OPENAI_API_KEY)
llm_chain = LLMChain(
    llm = llm,
    prompt=prompt,
    memory=cass_buff_memory
)


# So what will happen is ..this LLMChain will take the memory buffer(cass_buff_memory) and inject it inside the prompt as the
# {chat_history} key...and it is up to us to provide the {human_input}....and then according to this it will generate next 
# prompt...


# Getting the Prompt from the LLM...after this we will put this in a loop until the game ends...and we keep on generating next 
# prompt based on message_history....
# response = llm_chain.predict(human_input="start the game")
# print(response)


# Now defining this first choice as "start"...as we want the user to see the first message from the API itself..
choice = "start"
while(True):
    response = llm_chain.predict(human_input=choice)
    print(response.strip())
    
    if "The End." in response:
        break
    choice = input("Your Reply: ")












